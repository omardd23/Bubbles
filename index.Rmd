---
title: "Caso"
author: "Roberto Ruz Campos"
date: "30/11/2019"
output:
  html_document:
    df_print: paged
---

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(rvest)
```

Se importa la tabla "Compustat Global Daily" con los tipos de datos correctos.
```{r}
global_daily <- read_csv("Compustat_Global_Daily.csv",
  col_types = cols(
    sedol = col_character(),
    datadate = col_date(format = "%Y%m%d")
  )
)
```

Extraemos de Wikipedia la tabla de códigos GICS con sus respectivos nombres.

```{r message=FALSE, warning=FALSE}
gics_table <- read_html("https://en.wikipedia.org/wiki/Global_Industry_Classification_Standard") %>%
  html_node("table") %>%
  html_table(fill = TRUE) %>%
  as_tibble(.name_repair = "universal") %>%
  distinct(gsector = Sector...1, gics_name = Sector...2)
```

Se le añaden los nombres de los sectores a la tabla original
```{r}
global_daily <- left_join(global_daily, gics_table, by = "gsector")
```

Se añade una variable para identicar de manera única a cada registro.
```{r}
global_daily <- global_daily %>% 
  arrange(datadate) %>% 
  mutate(id = row_number())
```

Tipo de variable de cada columna
```{r}
global_daily %>%
  summarise_all(class) %>%
  pivot_longer(everything(), names_to = "column", values_to = "type")
```


## Exploratory analysis

Podemos notar que tenemos una tablas con `r dim(global_daily)[1]` filas y 
`r dim(global_daily)[2]` columnas.
  
¿Cuántos valores distintos hay por cada columna? y ¿ Cuántos `NA` hay en cada columna?
```{r}
global_daily %>% 
  summarise_all(n_distinct)

global_daily %>%
  summarise_all(~ sum(is.na(.x)))

```

Deberá llamarnos la atención que no hay correspondencia uno a uno entre `conm` y `gvkey`,
por lo que suponemos que un `conm` tiene 2 `gvkey`, pues hay 292 `gvkey` y 291 `conm` Veamos...

```{r}
global_daily %>% 
  select(gvkey, conm) %>% 
  group_by(conm) %>% 
  summarise(distinct_key = n_distinct(gvkey)) %>% 
  filter(distinct_key > 1)
```
## Caso Nacional Financiera 

Efectivamente es "NACIONAL FINANCIERA SNC" quien tiene asociados dos `gvkey` distintos.
Veamos cuales son
```{r}
global_daily %>% 
  filter(conm == "NACIONAL FINANCIERA SNC") %>% 
  group_by(gvkey) %>% 
  summarise(n = n(), prim_ap = min(datadate), ult_ap = max(datadate))
```
Lo que nos hace suponer que desde agosto de 2018 coexisten los dos `gvkey`, por lo que
tendría que haber más de un registro por día
```{r}
global_daily %>% 
  filter(conm == "NACIONAL FINANCIERA SNC") %>% 
  group_by(datadate) %>% 
  summarise(n = n()) %>% 
  filter(datadate >= "2013-08-07", datadate <= "2019-11-26")
```

Efectivamente cada día hay dos registros cada uno con diferente `gvkey`. Tomemos de muestra
al 2019.
```{r}

global_daily %>% 
  mutate(year = year(datadate)) %>% 
  filter(conm == "NACIONAL FINANCIERA SNC", year == 2019) %>% 
  arrange(datadate)

```

**¡¿Qué hacemos?!** 
¿Quitamos los 1,645 registros de gvkey 315924?

***

Siguiendo con el análisis... Vemos que hay cinco diferentes monedas `curcdd`y cinco diferentes
mercados `exchg`. 
Donde el código 208 corresponde a la bolsa mexicana y que tiene 708,038 que coinciden con 
la suma de los registros con las monedas MXP, MXN y los NA.
Así que todos los registros que no sean `exchg = 208 ` son candidatos a eliminación

```{r}
global_daily %>% 
  count(curcdd) 

global_daily %>% 
  count(exchg)

global_daily %>%
  count(curcdd) %>%
  filter(curcdd == "MXN" | curcdd == "MXP" | is.na(curcdd)) %>%
  summarise(sum(n)) %>%
  pull()
```
***

## observaciones de cada compañía
```{r}
global_daily %>% 
  count(conm) %>% 
  arrange(n)

# Emisoras con solo una observación
global_daily %>% 
  select(conm, datadate) %>% 
  group_by(conm) %>%
  mutate(n = n()) %>% 
  filter(n == 1)
```



¿Cuántas empresas hay por sector?
```{r}
global_daily %>% 
  group_by(gics_name) %>% 
  summarise(n = n_distinct(conm))
```

***

# Se comienza a limpiar
```{r}
# Se eliminan los registros que no correspondan al código de mercado de la BMV, así
# como el registro restante en USD 
global_daily1 <- global_daily %>% 
  filter(exchg == 208, !curcdd %in% "USD")

# Solo quedan registros en MXP, MXN y NA
global_daily1 %>% 
  count(curcdd)

# Se procede a ver cuales son los registros que tiene NA en moneda, se selecciona
# la fecha y emisora con el fin de ver si se tiene algún repetido que sí tenga moneda,
# para poder eliminar el que aparezca sin moneda
curcdd_na <- global_daily1 %>% 
  filter(is.na(curcdd)) %>% 
  mutate(dummy = paste(as.character(datadate), conm)) %>% 
  pull(dummy)

# Se selecciona los registros que tienen moneda y tienen un repetido sin moneda
rep_curr <- global_daily1 %>% 
  mutate(dummy = paste(as.character(datadate), conm)) %>% 
  filter(dummy %in% curcdd_na) %>% 
  filter(curcdd == "MXN") %>% 
  pull(dummy)

filter_out <-  global_daily1 %>% 
  mutate(dummy = paste(as.character(datadate), conm)) %>% 
  filter(dummy %in% rep_curr & is.na(curcdd)) %>% 
  pull(id)

global_daily1 <- global_daily1 %>% 
  filter(!id %in% filter_out)
# ---

# ¿Cuantos NA quedan?
global_daily1 %>% 
  count(curcdd)


# Puesto que los que restan con NA en curcdd también tienen NA en varias columnas más
# procedo a eliminarlas

global_daily1 <- global_daily1 %>% 
  filter(!is.na(curcdd))
```
# eliminar del global environment elim

```{r}
# Como se ha progresado con repecto a los NA
global_daily1 %>%
  summarise_all(~ sum(is.na(.x)))
```
***

Todas las empresas que tienen alguna fecha duplicada

```{r}
# Cuales son el combinado de fecha/empresa que aparecen en la base más de una vez
# y los extraemos hacia un vector
filt_dup <- global_daily1 %>% 
  group_by(datadate, conm) %>% 
  count(datadate) %>% 
  filter(n > 1)  %>% 
  mutate(dummy = paste(as.character(datadate),conm)) %>% 
  pull(dummy)

# Se crea un dataframe con los registros dupolicados
dups <- global_daily1 %>%
  mutate(dummy = paste(as.character(datadate), conm)) %>%
  filter(dummy %in% filt_dup)

dups 
```
¿Cuántas empresas diferentes tienen duplicados y cuáles son?
```{r}
length(unique(dups$conm))
unique(dups$conm)
```

Para la limpieza de duplicados tomaremos el registro con mayor mv = cshoc * prccd
Pero antes de proceder a ello tenemos que considerar los registros en que cshoc tiene
`NA`

```{r}
# Identificamos y extraemos a un vector la combinacion de fecha y empresa que tienen
# NA en cshoc para poder ver si tienen un duplicado que sí tenga un valor en cshoc
cshoc_na <- dups %>% 
  filter(is.na(cshoc)) %>% 
  pull(dummy)

dups %>% 
  filter(dummy %in% cshoc_na) 

# Se ve que todos los registros tienen alguno igual pero sin na en cshoc, entonces se
# pueden eliminar los que tienen na
dups %>% 
  filter(dummy %in% cshoc_na) %>%  
  group_by(datadate, conm) %>% 
  summarise(sin_na = sum(!is.na(cshoc)), con_na = sum(is.na(cshoc))) 

elim2 <- dups %>% 
  filter(dummy %in% cshoc_na, is.na(cshoc)) %>% 
  pull(id)
 
global_daily1 <- global_daily1 %>% 
  filter(!id %in% elim2)

# Se actualiza el datafrane
dups <- dups %>% 
  filter(!id %in% elim2)

dups %>% 
  summarise_all(~ sum(is.na(.x)))

```


```{r}
dups <- dups %>% 
  mutate(mv = cshoc * prccd)

dups
```

```{r}
dups %>% 
  group_by(datadate, conm) %>% 
  count() 

dups <- dups %>% 
  group_by(datadate, conm) %>% 
  mutate(rank = rank(-mv)) # %>% 
  #filter(rank > 1.5)

elim3 <- dups %>% 
  filter(rank > 1.5) %>% 
  pull(id)

#### Checkpoint

global_daily1 <- global_daily1 %>% 
  filter(!id %in% elim3)
#####

#Los duplicados que quedan. Actualizamos dups
dups <- dups %>% 
  filter(rank == 1.5)  # será fácil quitar los que no tengan isin
dups

```
```{r}
dups %>% 
  count(datadate,conm)

dups %>% 
 summarise(na = sum(is.na(isin)), not_na = sum(!is.na(isin)))

elim4 <- dups %>% 
  filter(is.na(isin)) %>% 
  pull(id)

#### Checkpoint
global_daily1 <- global_daily1 %>% 
  filter(!id %in% elim4)
####


#Actualiza dups

vec_dups <- dups %>% 
 summarise(na = sum(is.na(isin)), not_na = sum(!is.na(isin))) %>% 
  filter(na == 0) %>% 
  mutate(dummy = paste(as.character(datadate), conm)) %>% 
  pull(dummy)

dups <- dups %>% 
  filter(dummy %in% vec_dups)

dups
```

```{r}
global_daily1 %>% 
  group_by(datadate, conm) %>% 
  count(datadate) %>% 
  filter(n > 1) 


global_daily1 %>%
  summarise_all(~ sum(is.na(.x)))

global_daily1 %>% 
  count(conm) %>% 
  arrange(n)

```

```{r}
global_daily1 %>% 
  mutate(year = year(datadate)) %>% 
  group_by(conm, year) %>% 
  summarise(n = n()) %>% 
  spread(year, n)
```

